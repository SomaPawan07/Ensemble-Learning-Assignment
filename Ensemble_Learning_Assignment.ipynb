{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it?**\n",
        "- Ensemble Learning in machine learning refers to a technique that combines predictions from multiple models (often called base learners or weak learners) to produce a more accurate and robust overall model.\n",
        "\n",
        "Key Idea Behind Ensemble Learning:\n",
        "The collective wisdom of multiple models is often better than the prediction of any single model.\n",
        "\n",
        "Instead of relying on one model that may have limitations or biases, ensemble methods harness the strengths of several models to improve prediction accuracy, reduce overfitting, and increase generalization."
      ],
      "metadata": {
        "id": "sYrbTSCRHJBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. What is the difference between Bagging and Boosting ?**\n",
        "- The key difference between Bagging and Boosting lies in how they build and combine multiple models, and what they focus on improving.\n",
        "\n",
        "| Feature                   | **Bagging**                                                        | **Boosting**                                                                           |\n",
        "| ------------------------- | ------------------------------------------------------------------ | -------------------------------------------------------------------------------------- |\n",
        "| **Full Name**             | Bootstrap Aggregating                                              | Boosting                                                                               |\n",
        "| **Goal**                  | Reduce **variance** (prevent overfitting)                          | Reduce **bias** (improve accuracy)                                                     |\n",
        "| **Model Training**        | Models are trained **independently and in parallel**               | Models are trained **sequentially**, each one learning from the errors of the previous |\n",
        "| **Data Sampling**         | Uses **random subsets** of the data (with replacement)             | Trains on the **full dataset**, but gives more weight to misclassified instances       |\n",
        "| **Model Weighting**       | All models usually have **equal weight** in the final output       | Models are **weighted** based on performance (better models have more influence)       |\n",
        "| **Combining Predictions** | **Majority voting** (classification) or **averaging** (regression) | **Weighted voting** or **weighted sum** of predictions                                 |\n",
        "| **Overfitting Risk**      | Less likely (due to randomization)                                 | More likely if overfitting to noise, but can be controlled                             |\n",
        "| **Examples**              | Random Forest, Bagged Decision Trees                               | AdaBoost, Gradient Boosting, XGBoost, LightGBM                                         |\n"
      ],
      "metadata": {
        "id": "Z3YCOWGxI7eo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. What is bootstrap sampling and what role does it play in Bagging methods like Random Forest ?**\n",
        "- Bootstrap sampling is a statistical technique used to generate multiple random datasets from a single original dataset by sampling with replacement.\n",
        "- We take the original dataset of size n.\n",
        "- We randomly select data points with replacement to create a new dataset of the same size (n).\n",
        "- Some samples may appear multiple times, others not at all.\n",
        "- Each such dataset is called a bootstrap sample.\n",
        "\n",
        "Bagging stands for Bootstrap Aggregating, and bootstrap sampling is the first step in this process.\n",
        "\n",
        "- Bootstrap Sampling: For each decision tree in the forest, create a different bootstrap sample from the original training data.\n",
        "- Model Training: Each tree is trained independently on its own bootstrap sample.\n",
        "- Aggregation:\n",
        "For prediction:\n",
        "- Classification: Use majority voting from all trees.\n",
        "- Regression: Use the average of all tree predictions."
      ],
      "metadata": {
        "id": "VZ9dDN7EJMwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?**\n",
        "- In Bagging methods like Random Forest, each model (e.g. decision tree) is trained on a bootstrap sample — a random sample with replacement from the original dataset.\n",
        "  - Since the sampling is with replacement, about 63% of the original data points end up in each bootstrap sample.\n",
        "  - The remaining ~37% of the data points are not included in that bootstrap sample — these are called Out-of-Bag (OOB) samples.\n",
        "\n",
        "OOB score is useful in many ways:\n",
        "- Acts as a built-in cross-validation for bagging methods.\n",
        "- Saves time and computational cost — no need to set aside a validation set.\n",
        "- Gives a reliable estimate of model performance, especially in large datasets.\n",
        "- Helps in hyperparameter tuning (e.g., choosing number of trees)."
      ],
      "metadata": {
        "id": "MsMvij4QJloI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest?**\n",
        "- Here's a direct comparison of how feature importance is calculated and interpreted in a single Decision Tree vs. a Random Forest:\n",
        "\n",
        "1. Single Decision Tree – Feature Importance\n",
        "\n",
        "How it's calculated:\n",
        "   -  Based on how much each feature reduces impurity (like Gini impurity or entropy for classification, variance for regression) when it's used to split nodes.\n",
        "\n",
        "  -  The importance of a feature is the sum of all decreases in impurity across all nodes where that feature is used.\n",
        "\n",
        "Pros:\n",
        "- Fast and easy to compute.\n",
        "- Interpretability: You can trace decisions through the tree to understand why a feature matters.\n",
        "\n",
        "Cons:\n",
        "- High variance: Results depend heavily on the particular training data.\n",
        "- May overemphasize features that cause large splits in early nodes.\n",
        "- Sensitive to noise and overfitting.\n",
        "\n",
        "2. Random Forest – Feature Importance\n",
        " How it's calculated:\n",
        "- Similar to a single tree, but the importance is averaged across all trees in the forest.\n",
        "- For each tree:\n",
        "   -    Track the reduction in impurity caused by each feature.\n",
        "   -   Aggregate and average over all trees to get the final feature importance scores.\n",
        "\n",
        "Pros:\n",
        "   -   More stable and reliable than a single tree (less sensitive to data noise).\n",
        "   -  Better at capturing true signal by averaging over many trees.\n",
        "   -  Works well even when features are correlated or redundant.\n",
        "\n",
        "Cons:\n",
        "    \n",
        "- Less interpretable than a single tree."
      ],
      "metadata": {
        "id": "W-5FjdvPLLFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. Write a Python program to:**\n",
        "\n",
        "● **Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()**\n",
        "\n",
        "● **Train a Random Forest Classifier**\n",
        "\n",
        "● **Print the top 5 most important features based on feature importance scores.**"
      ],
      "metadata": {
        "id": "_AABQLWCM_rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "importances = model.feature_importances_\n",
        "\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': data.feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "top_5_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(5)\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top_5_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdHjYroQN1UA",
        "outputId": "44a09fa4-b9dc-4093-f5af-6eea0985e692"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. Write a Python program to:**\n",
        "\n",
        "● **Train a Bagging Classifier using Decision Trees on the Iris dataset**\n",
        "\n",
        "● **Evaluate its accuracy and compare with a single Decision Tree**"
      ],
      "metadata": {
        "id": "nCkZLGMYOCZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "dtree = DecisionTreeClassifier(random_state=42)\n",
        "dtree.fit(X_train, y_train)\n",
        "dtree_preds = dtree.predict(X_test)\n",
        "dtree_accuracy = accuracy_score(y_test, dtree_preds)\n",
        "\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bagging.fit(X_train, y_train)\n",
        "bagging_preds = bagging.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_preds)\n",
        "\n",
        "print(\"Single Decision Tree Accuracy:\", round(dtree_accuracy, 4))\n",
        "print(\"Bagging Classifier Accuracy:  \", round(bagging_accuracy, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpzdvWzMO0pu",
        "outputId": "b3b5df8d-8345-41ea-8839-c5b0bfce4bae"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy:   1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. Write a Python program to:**\n",
        "\n",
        "● **Train a Random Forest Classifier**\n",
        "\n",
        "● **Tune hyperparameters max_depth and n_estimators using GridSearchCV**\n",
        "\n",
        "● **Print the best parameters and final accuracy**"
      ],
      "metadata": {
        "id": "lKzmNSGUQTVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'max_depth': [None, 3, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Final Accuracy on Test Set:\", round(accuracy, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w31vaBNMQs_F",
        "outputId": "63b40c10-b711-4fee-d6bf-ba5b4a3341ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Accuracy on Test Set: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9. Write a Python program to:**\n",
        "\n",
        "● **Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset**\n",
        "\n",
        "● **Compare their Mean Squared Errors (MSE)**"
      ],
      "metadata": {
        "id": "ekbZbeOlaBHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "bagging_model = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bagging_model.fit(X_train, y_train)\n",
        "bagging_preds = bagging_model.predict(X_test)\n",
        "bagging_mse = mean_squared_error(y_test, bagging_preds)\n",
        "\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_preds = rf_model.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_preds)\n",
        "\n",
        "print(\"Bagging Regressor MSE:       \", round(bagging_mse, 4))\n",
        "print(\"Random Forest Regressor MSE: \", round(rf_mse, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGHt8ek_aswY",
        "outputId": "67b844d1-767b-4feb-bc50-04c8840936c7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE:        0.2579\n",
            "Random Forest Regressor MSE:  0.2577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10. You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.**\n",
        "\n",
        "**You decide to use ensemble techniques to increase model performance.**\n",
        "\n",
        "**Explain your step-by-step approach to:**\n",
        "\n",
        "● **Choose between Bagging or Boosting**\n",
        "\n",
        "● **Handle overfitting**\n",
        "\n",
        "● **Select base models**\n",
        "\n",
        "● **Evaluate performance using cross-validation**\n",
        "\n",
        "● **Justify how ensemble learning improves decision-making in this real-world context.**"
      ],
      "metadata": {
        "id": "A04yu1t9bU5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Step-by-Step Approach\n",
        " 1. Choose Between Bagging or Boosting\n",
        "Key Considerations:\n",
        "- Bagging (e.g., Random Forest):\n",
        "- Reduces variance\n",
        "- Best for unstable models (e.g., decision trees)\n",
        "- Works well if model is overfitting or if data is noisy\n",
        "- Boosting (e.g., XGBoost, LightGBM):\n",
        "- Reduces bias\n",
        "- Builds strong learners by correcting mistakes from previous models\n",
        "- Works better when the model is underfitting or needs fine-tuning\n",
        "\n",
        "2. Handle Overfitting\n",
        "- Boosting models are powerful but prone to overfitting, especially on noisy or high-dimensional data.\n",
        "- Techniques:\n",
        "     - Early stopping: Stop training when validation error stops improving\n",
        "- Regularization:\n",
        "\n",
        "     - max_depth (limit tree depth)\n",
        "     - learning_rate (controls step size)\n",
        "     - subsample (use a fraction of data per tree)\n",
        "     - colsample_bytree (use a fraction of features per tree)\n",
        "- Cross-validation to detect overfitting early\n",
        "     - Feature selection / dimensionality reduction (e.g., remove irrelevant or highly correlated features)\n",
        "\n",
        "3. Select Base Models\n",
        "Bagging: Use DecisionTreeClassifier as the base estimator (high variance → good for bagging)\n",
        "Boosting:\n",
        "- Base learners are typically shallow trees (e.g., max_depth=3) to reduce overfitting\n",
        "- Algorithms like XGBoost, LightGBM, and CatBoost implement this internally\n",
        "- For boosting, no need to manually specify the base model — the framework uses efficient trees already\n",
        "\n",
        "Evaluate Performance Using Cross-Validation\n",
        "Process:\n",
        "- Use Stratified K-Fold Cross-Validation to maintain class balance (important for imbalanced loan default data)\n",
        "- Evaluation metrics:\n",
        "- AUC-ROC (discrimination between default vs. no-default)\n",
        "- Precision-Recall (especially useful with imbalanced data)\n",
        "- F1-score, Confusion Matrix\n",
        "\n",
        "Optionally, use cost-based evaluation (false negatives might be costlier)\n",
        "\n",
        "Implementation:"
      ],
      "metadata": {
        "id": "v5wAyaJPcRNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = cross_val_score(model, X, y, cv=cv, scoring='roc_auc')\n",
        "print(\"Average AUC-ROC:\", scores.mean())\n"
      ],
      "metadata": {
        "id": "W1fbQARjdfea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Justify Ensemble Learning in the Real-World Context\n",
        "Why Ensemble Learning Helps in Loan Default Prediction:\n",
        "\n",
        "| Challenge               | How Ensemble Helps                                                           |\n",
        "| ----------------------- | ---------------------------------------------------------------------------- |\n",
        "| High-dimensional data   | Learns complex interactions through many trees                               |\n",
        "| Imbalanced classes      | Boosting can focus more on hard-to-classify (minority) samples               |\n",
        "| Noise in features       | Bagging averages out instability caused by noisy samples                     |\n",
        "| Interpretability needed | Feature importance from models like Random Forest or XGBoost aids compliance |\n",
        "| Cost of wrong decisions | Boosting minimizes critical errors by iteratively improving predictions      |\n",
        "\n",
        "\n",
        "Impact:\n",
        "By increasing predictive accuracy, reducing false negatives, and offering more reliable risk assessments, ensemble models directly support better credit decisioning, fraud detection, and risk mitigation — all essential in a financial institution."
      ],
      "metadata": {
        "id": "Dtr8RU3ldgnO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6esUwE-HBm1"
      },
      "outputs": [],
      "source": []
    }
  ]
}